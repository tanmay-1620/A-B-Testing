{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'D:\\AB Testing\\data\\test_data_A.csv\\test_data_A.csv')\n",
    "\n",
    "# Take the first half of the dataset (500,000 rows if total rows = 1,000,000)\n",
    "df_half = df.head(500000)\n",
    "\n",
    "# Save the sampled half (optional)\n",
    "df_half.to_csv('half_of_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: Index(['id', 'uid', 'task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id',\n",
      "       'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags',\n",
      "       'app_first_class', 'app_second_class', 'age', 'city', 'city_rank',\n",
      "       'device_name', 'device_size', 'career', 'gender', 'net_type',\n",
      "       'residence', 'his_app_size', 'his_on_shelf_time', 'app_score',\n",
      "       'emui_dev', 'list_time', 'device_price', 'up_life_duration',\n",
      "       'up_membership_grade', 'membership_life_duration', 'consume_purchase',\n",
      "       'communication_onlinerate', 'communication_avgonline_30d', 'indu_name',\n",
      "       'pt_d'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset using the correct delimiter\n",
    "df = pd.read_csv(r'D:\\AB Testing\\data\\half_of_data.csv', delimiter='|')\n",
    "\n",
    "# Check the column names again\n",
    "print(\"Columns in dataset:\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values found in communication_onlinerate:\n",
      "0    3^4^5^6^7^8^9^10^11^12^13^14^15^16^17^18^19^20...\n",
      "1      7^8^9^10^11^12^13^14^15^16^17^18^19^20^21^22^23\n",
      "2    5^6^7^8^9^10^11^12^13^14^15^16^17^18^19^20^21^...\n",
      "3    6^7^8^9^10^11^12^13^14^15^16^17^18^19^20^21^22^23\n",
      "4      7^8^9^10^11^12^13^14^15^16^17^18^19^20^21^22^23\n",
      "Name: communication_onlinerate, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tanmay\\AppData\\Local\\Temp\\ipykernel_14780\\2798828310.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['communication_onlinerate'].fillna(df['communication_onlinerate'].median(), inplace=True)\n",
      "C:\\Users\\Tanmay\\AppData\\Local\\Temp\\ipykernel_14780\\2798828310.py:38: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['city_rank'].fillna(df['city_rank'].median(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned successfully!\n",
      "Data cleaned successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify columns that should be numeric\n",
    "numerical_columns = ['age', 'city_rank', 'app_score', 'device_price', \n",
    "                     'up_life_duration', 'up_membership_grade', \n",
    "                     'membership_life_duration', 'communication_onlinerate', \n",
    "                     'communication_avgonline_30d']\n",
    "\n",
    "# Step 2: Check for any non-numeric values in the numerical columns\n",
    "for col in numerical_columns:\n",
    "    # Check if any value is not a number\n",
    "    non_numeric_rows = df[~df[col].apply(pd.to_numeric, errors='coerce').notna()]\n",
    "    \n",
    "    if not non_numeric_rows.empty:\n",
    "        print(f\"Non-numeric values found in {col}:\")\n",
    "        print(non_numeric_rows[col].head())\n",
    "\n",
    "\n",
    "# Step 1: Replace the '^' separated values with the first number in the sequence\n",
    "df['communication_onlinerate'] = df['communication_onlinerate'].apply(lambda x: x.split('^')[0] if isinstance(x, str) else x)\n",
    "\n",
    "# Step 2: Convert the column to numeric\n",
    "df['communication_onlinerate'] = pd.to_numeric(df['communication_onlinerate'], errors='coerce')\n",
    "\n",
    "# Step 3: Handle missing values (NaN) in the column\n",
    "df['communication_onlinerate'].fillna(df['communication_onlinerate'].median(), inplace=True)\n",
    "\n",
    "# Step 4: Proceed with the rest of the preprocessing (scaling, etc.)\n",
    "scaler = StandardScaler()\n",
    "df[['communication_onlinerate']] = scaler.fit_transform(df[['communication_onlinerate']])\n",
    "\n",
    "print(\"Data cleaned successfully!\")\n",
    "\n",
    "\n",
    "# Step 3: Fix the issue\n",
    "# For example, if 'city_rank' has malformed entries like '3^4^5^6...', we can remove those rows or replace them with NaN\n",
    "df['city_rank'] = pd.to_numeric(df['city_rank'], errors='coerce')\n",
    "\n",
    "# Optionally, fill NaN values with the median or drop rows with NaN values\n",
    "df['city_rank'].fillna(df['city_rank'].median(), inplace=True)\n",
    "\n",
    "# Step 4: Proceed with the rest of the preprocessing\n",
    "# Now continue with scaling or other preprocessing steps\n",
    "scaler = StandardScaler()\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "print(\"Data cleaned successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8877\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.89      1.00      0.94     87866\n",
      "           3       0.88      0.04      0.07       917\n",
      "           4       1.00      0.05      0.10       111\n",
      "           5       0.99      0.05      0.09      5667\n",
      "           6       0.90      0.05      0.10       169\n",
      "           7       0.80      0.06      0.11      1014\n",
      "           8       0.89      0.05      0.09       664\n",
      "           9       0.92      0.05      0.09       240\n",
      "          10       0.64      0.15      0.24      3352\n",
      "\n",
      "    accuracy                           0.89    100000\n",
      "   macro avg       0.88      0.17      0.20    100000\n",
      "weighted avg       0.89      0.89      0.84    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ===========================\n",
    "# 1. Categorical Encoding\n",
    "# ===========================\n",
    "\n",
    "# List of categorical columns\n",
    "cat_cols = ['creat_type_cd', 'adv_prim_id', 'inter_type_cd', 'slot_id', 'spread_app_id', \n",
    "            'app_first_class', 'app_second_class', 'city', 'city_rank', 'device_name', \n",
    "            'career', 'gender', 'net_type', 'residence', 'emui_dev', 'indu_name', 'pt_d']\n",
    "\n",
    "# One-Hot Encoding for low-cardinality columns\n",
    "# One-Hot Encoding for low-cardinality columns\n",
    "ohe_cols = ['gender', 'net_type', 'city_rank']\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "ohe_df = pd.DataFrame(ohe.fit_transform(df[ohe_cols]), columns=ohe.get_feature_names_out(ohe_cols))\n",
    "df = pd.concat([df.drop(columns=ohe_cols), ohe_df], axis=1)\n",
    "\n",
    "\n",
    "# Label Encoding for high-cardinality columns\n",
    "le_cols = list(set(cat_cols) - set(ohe_cols))\n",
    "for col in le_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# ===========================\n",
    "# 2. Feature Scaling\n",
    "# ===========================\n",
    "\n",
    "# List of numeric columns\n",
    "num_cols = ['age', 'device_size', 'his_app_size', 'his_on_shelf_time', 'app_score', \n",
    "            'list_time', 'device_price', 'up_life_duration', 'up_membership_grade', \n",
    "            'membership_life_duration', 'communication_onlinerate', 'communication_avgonline_30d']\n",
    "\n",
    "# Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# ===========================\n",
    "# 3. Target Variable\n",
    "# ===========================\n",
    "\n",
    "# Define target and features\n",
    "X = df.drop(columns=['consume_purchase'])\n",
    "y = df['consume_purchase']\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ===========================\n",
    "# 4. Model Training\n",
    "# ===========================\n",
    "\n",
    "# Random Forest Classifier (baseline model)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ===========================\n",
    "# 5. Model Evaluation\n",
    "# ===========================\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled class distribution:\n",
      "consume_purchase\n",
      "2     351356\n",
      "5     351356\n",
      "3     351356\n",
      "10    351356\n",
      "7     351356\n",
      "6     351356\n",
      "4     351356\n",
      "9     351356\n",
      "8     351356\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['consume_purchase'])\n",
    "y = df['consume_purchase']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check resampled class distribution\n",
    "print(\"Resampled class distribution:\")\n",
    "print(y_train_resampled.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88744\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.89      1.00      0.94     87839\n",
      "           3       0.96      0.05      0.10       926\n",
      "           4       1.00      0.05      0.10        95\n",
      "           5       0.96      0.05      0.10      5629\n",
      "           6       0.92      0.07      0.13       175\n",
      "           7       0.88      0.06      0.12      1065\n",
      "           8       0.87      0.08      0.15       650\n",
      "           9       1.00      0.07      0.14       241\n",
      "          10       0.67      0.14      0.23      3380\n",
      "\n",
      "    accuracy                           0.89    100000\n",
      "   macro avg       0.91      0.18      0.22    100000\n",
      "weighted avg       0.89      0.89      0.84    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define and train the model\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (500000, 36)\n",
      "Columns in dataset: Index(['id', 'uid', 'task_id', 'adv_id', 'creat_type_cd', 'adv_prim_id',\n",
      "       'dev_id', 'inter_type_cd', 'slot_id', 'spread_app_id', 'tags',\n",
      "       'app_first_class', 'app_second_class', 'age', 'city', 'city_rank',\n",
      "       'device_name', 'device_size', 'career', 'gender', 'net_type',\n",
      "       'residence', 'his_app_size', 'his_on_shelf_time', 'app_score',\n",
      "       'emui_dev', 'list_time', 'device_price', 'up_life_duration',\n",
      "       'up_membership_grade', 'membership_life_duration', 'consume_purchase',\n",
      "       'communication_onlinerate', 'communication_avgonline_30d', 'indu_name',\n",
      "       'pt_d'],\n",
      "      dtype='object')\n",
      "Original class distribution:\n",
      " consume_purchase\n",
      "2     439195\n",
      "5      28145\n",
      "10     16900\n",
      "7       5323\n",
      "3       4629\n",
      "8       3251\n",
      "9       1205\n",
      "6        876\n",
      "4        476\n",
      "Name: count, dtype: int64\n",
      "Resampled class distribution:\n",
      " consume_purchase\n",
      "2     351356\n",
      "5     351356\n",
      "3     351356\n",
      "10    351356\n",
      "7     351356\n",
      "6     351356\n",
      "4     351356\n",
      "9     351356\n",
      "8     351356\n",
      "Name: count, dtype: int64\n",
      "✅ Accuracy: 0.88631\n",
      "📊 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           2       0.89      1.00      0.94     87839\n",
      "           3       0.96      0.05      0.09       926\n",
      "           4       1.00      0.03      0.06        95\n",
      "           5       0.69      0.05      0.10      5629\n",
      "           6       1.00      0.07      0.13       175\n",
      "           7       0.75      0.06      0.12      1065\n",
      "           8       0.78      0.09      0.16       650\n",
      "           9       0.88      0.10      0.17       241\n",
      "          10       0.60      0.15      0.24      3380\n",
      "\n",
      "    accuracy                           0.89    100000\n",
      "   macro avg       0.84      0.18      0.22    100000\n",
      "weighted avg       0.87      0.89      0.84    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "# Ensure that the delimiter is correctly set to match your data\n",
    "file_path = r\"D:\\AB Testing\\data\\half_of_data.csv\"\n",
    "df = pd.read_csv(file_path, delimiter='|')\n",
    "\n",
    "# Print shape and column names\n",
    "print(\"Shape of dataset:\", df.shape)\n",
    "print(\"Columns in dataset:\", df.columns)\n",
    "\n",
    "# Handle concatenated or string columns if needed\n",
    "for col in df.columns:\n",
    "    # Check if the column contains non-numeric data\n",
    "    if df[col].dtype == 'object':\n",
    "        # If the column has concatenated values, split and take the first one\n",
    "        df[col] = df[col].astype(str).str.split('^').str[0]\n",
    "        # Attempt to convert to numeric if possible\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Drop any columns that still have NaN or non-numeric data after conversion\n",
    "df = df.dropna()\n",
    "\n",
    "# Check for final non-numeric columns\n",
    "non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "if len(non_numeric_cols) > 0:\n",
    "    print(f\"Non-numeric columns found: {non_numeric_cols}\")\n",
    "    # Label encode remaining categorical columns\n",
    "    le = LabelEncoder()\n",
    "    for col in non_numeric_cols:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['consume_purchase'])\n",
    "y = df['consume_purchase']\n",
    "\n",
    "# Check target distribution\n",
    "print(\"Original class distribution:\\n\", y.value_counts())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Apply SMOTE to balance the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check resampled class distribution\n",
    "print(\"Resampled class distribution:\\n\", y_train_resampled.value_counts())\n",
    "\n",
    "# Initialize and train RandomForest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"📊 Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns to fix:\n",
      " Index(['communication_onlinerate'], dtype='object')\n",
      "Resampled class distribution:\n",
      " 0    2753\n",
      "2    2753\n",
      "7    2753\n",
      "3    2753\n",
      "6    2753\n",
      "4    2753\n",
      "1    2753\n",
      "5    2753\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.9404600811907984\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           2       0.95      0.99      0.97       688\n",
      "           3       1.00      0.33      0.50         3\n",
      "           5       0.67      0.17      0.28        23\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       1.00      0.60      0.75         5\n",
      "           8       0.50      0.20      0.29         5\n",
      "           9       0.00      0.00      0.00         1\n",
      "          10       0.44      0.31      0.36        13\n",
      "\n",
      "    accuracy                           0.94       739\n",
      "   macro avg       0.57      0.33      0.39       739\n",
      "weighted avg       0.93      0.94      0.93       739\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ✅ Load the dataset with correct delimiter\n",
    "file_path = r\"D:\\AB Testing\\data\\half_of_data.csv\"  # Update your file path\n",
    "df = pd.read_csv(file_path, sep='|')  # Correct delimiter for your file\n",
    "\n",
    "# ✅ Clean and check column names\n",
    "df.columns = df.columns.str.strip()  # Remove any leading/trailing spaces\n",
    "\n",
    "# ✅ Drop unnecessary columns if they exist\n",
    "drop_cols = ['pt_d', 'id']  # Drop 'pt_d' and 'id' if they exist\n",
    "df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "# ✅ Identify non-numeric columns to fix\n",
    "non_numeric_cols = df.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns to fix:\\n\", non_numeric_cols)\n",
    "\n",
    "# ✅ Convert problematic string columns to numeric (if possible)\n",
    "for col in non_numeric_cols:\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric if possible\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ✅ Handle remaining non-numeric columns using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for col in non_numeric_cols:\n",
    "    if df[col].dtype == 'object':  # Apply LabelEncoder only on remaining string columns\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# ✅ Drop any rows with NaN after conversion\n",
    "df = df.dropna()\n",
    "\n",
    "# ✅ Define features (X) and target (y)\n",
    "X = df.drop(columns=['consume_purchase'], errors='ignore')\n",
    "y = df['consume_purchase']\n",
    "\n",
    "# ✅ Encode target labels using LabelEncoder\n",
    "le_y = LabelEncoder()\n",
    "y = le_y.fit_transform(y)  # Map labels to [0, 1, 2, ...]\n",
    "\n",
    "# ✅ Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Fix SMOTE by dynamically adjusting k_neighbors\n",
    "min_class_count = np.min(np.bincount(y_train))  # Smallest class size\n",
    "k_neighbors = min(5, min_class_count - 1) if min_class_count > 1 else 1\n",
    "\n",
    "# ✅ Apply SMOTE with dynamically adjusted k_neighbors\n",
    "smote = SMOTE(k_neighbors=k_neighbors, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# ✅ Check resampled class distribution\n",
    "print(\"Resampled class distribution:\\n\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# ✅ Initialize XGBoost classifier with optimal parameters\n",
    "model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric=\"mlogloss\"\n",
    ")\n",
    "\n",
    "# ✅ Fit the model on resampled data\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# ✅ Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ✅ Decode predictions back to original classes\n",
    "y_pred_original = le_y.inverse_transform(y_pred)\n",
    "y_test_original = le_y.inverse_transform(y_test)\n",
    "\n",
    "# ✅ Evaluate model performance\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test_original, y_pred_original))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_original, y_pred_original))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns to fix:\n",
      " Index(['communication_onlinerate'], dtype='object')\n",
      "⚡️ Applying ADASYN due to high imbalance...\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanmay\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [13:39:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Best Parameters Found: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 500, 'subsample': 0.8}\n",
      "\n",
      "⚡️ Top 10 Important Features:\n",
      "                 Feature  Importance\n",
      "23            app_score    0.098253\n",
      "28  up_membership_grade    0.096767\n",
      "27     up_life_duration    0.069389\n",
      "10      app_first_class    0.065743\n",
      "26         device_price    0.057308\n",
      "17               career    0.049832\n",
      "16          device_size    0.040908\n",
      "12                  age    0.037515\n",
      "6         inter_type_cd    0.033784\n",
      "20            residence    0.032499\n",
      "✅ Model saved at: D:\\AB Testing\\models\\best_xgb_model.pkl\n",
      "\n",
      "🎯 Accuracy: 0.9981857764876633\n",
      "\n",
      "📊 Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           2       0.99      0.99      0.99       688\n",
      "           3       1.00      1.00      1.00       689\n",
      "           5       1.00      0.99      1.00       694\n",
      "           6       1.00      1.00      1.00       688\n",
      "           7       1.00      1.00      1.00       689\n",
      "           8       1.00      1.00      1.00       688\n",
      "           9       1.00      1.00      1.00       688\n",
      "          10       1.00      1.00      1.00       688\n",
      "\n",
      "    accuracy                           1.00      5512\n",
      "   macro avg       1.00      1.00      1.00      5512\n",
      "weighted avg       1.00      1.00      1.00      5512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ✅ Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib  # To save the model for real-time use\n",
    "\n",
    "# ✅ Load the dataset with correct delimiter\n",
    "file_path = r\"D:\\AB Testing\\data\\half_of_data.csv\"  # Update your file path\n",
    "df = pd.read_csv(file_path, sep='|')  # Correct delimiter as mentioned before\n",
    "\n",
    "# ✅ Clean and check column names\n",
    "df.columns = df.columns.str.strip()  # Strip leading/trailing spaces from columns\n",
    "\n",
    "# ✅ Drop unnecessary columns if they exist\n",
    "drop_cols = ['pt_d', 'id']  # Drop columns if present\n",
    "df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "# ✅ Handle non-numeric columns dynamically\n",
    "non_numeric_cols = df.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns to fix:\\n\", non_numeric_cols)\n",
    "\n",
    "# ✅ Convert problematic string columns to numeric if possible\n",
    "for col in non_numeric_cols:\n",
    "    try:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ✅ Label encode remaining non-numeric columns\n",
    "le = LabelEncoder()\n",
    "for col in non_numeric_cols:\n",
    "    if df[col].dtype == 'object':  # Apply LabelEncoder only to remaining object columns\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "# ✅ Drop rows with NaN after conversion\n",
    "df = df.dropna()\n",
    "\n",
    "# ✅ Define features (X) and target (y)\n",
    "X = df.drop(columns=['consume_purchase'], errors='ignore')\n",
    "y = df['consume_purchase']\n",
    "\n",
    "# ✅ Encode target labels using LabelEncoder\n",
    "le_y = LabelEncoder()\n",
    "y = le_y.fit_transform(y)\n",
    "\n",
    "# ✅ Handle cases with only one class (avoid SMOTE error)\n",
    "if len(np.unique(y)) == 1:\n",
    "    print(\"⚠️ Only one class present. Skipping resampling...\")\n",
    "    X_resampled, y_resampled = X, y\n",
    "else:\n",
    "    # ✅ Apply ADASYN/SMOTE dynamically based on class balance\n",
    "    min_class_count = np.min(np.bincount(y))\n",
    "    k_neighbors = min(5, min_class_count - 1) if min_class_count > 1 else 1\n",
    "\n",
    "    # Use ADASYN if classes are highly imbalanced\n",
    "    if min_class_count <= 10:\n",
    "        print(\"⚡️ Applying ADASYN due to high imbalance...\")\n",
    "        resampler = ADASYN(n_neighbors=k_neighbors, random_state=42)\n",
    "    else:\n",
    "        print(\"🔄 Applying SMOTE...\")\n",
    "        resampler = SMOTE(k_neighbors=k_neighbors, random_state=42)\n",
    "\n",
    "    # ✅ Resampling to balance classes\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X, y)\n",
    "\n",
    "# ✅ Split resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, stratify=y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# ✅ Define XGBoost classifier with base params\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='multi:softmax',  # Multi-class classification\n",
    "    eval_metric=\"mlogloss\",\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Define hyperparameter grid for optimization\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "# ✅ Perform GridSearchCV to find the best parameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ✅ Fit GridSearchCV to find the best model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# ✅ Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"🎯 Best Parameters Found: {grid_search.best_params_}\")\n",
    "\n",
    "# ✅ Feature importance analysis for optimization\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\\n⚡️ Top 10 Important Features:\\n\", feature_importances.head(10))\n",
    "\n",
    "# ✅ Save the best model for real-time use\n",
    "model_path = r\"D:\\AB Testing\\models\\best_xgb_model.pkl\"\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"✅ Model saved at: {model_path}\")\n",
    "\n",
    "# ✅ Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# ✅ Decode predictions back to original classes\n",
    "y_pred_original = le_y.inverse_transform(y_pred)\n",
    "y_test_original = le_y.inverse_transform(y_test)\n",
    "\n",
    "# ✅ Evaluate model performance\n",
    "print(\"\\n🎯 Accuracy:\", accuracy_score(y_test_original, y_pred_original))\n",
    "print(\"\\n📊 Classification Report:\\n\", classification_report(y_test_original, y_pred_original))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
